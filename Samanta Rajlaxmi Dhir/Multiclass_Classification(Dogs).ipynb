{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jessicali9530/stanford-dogs-dataset\n",
        "!unzip -q stanford-dogs-dataset.zip -d stanford-dogs-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2LLpWXmk7FL",
        "outputId": "3ba8a021-648f-42b3-8f32-7e86208a939a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset\n",
            "License(s): other\n",
            "Downloading stanford-dogs-dataset.zip to /content\n",
            " 99% 740M/750M [00:08<00:00, 73.8MB/s]\n",
            "100% 750M/750M [00:08<00:00, 88.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls stanford-dogs-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y93j_z-zlErb",
        "outputId": "73f4704d-cabb-4ecd-fe69-4189d5edfaaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations  images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "tehzmEyElHRF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "jmzZUS3UlJh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths (update these paths based on your setup)\n",
        "dataset_path = 'stanford-dogs-dataset'\n",
        "images_path = os.path.join(dataset_path, \"images\")\n",
        "annotations_path = os.path.join(dataset_path, \"annotation\")"
      ],
      "metadata": {
        "id": "AcJs9ButlNtb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "dataset_dir = 'stanford-dogs-dataset/images/Images'\n",
        "annotations_dir = 'stanford-dogs-dataset/annotations/Annotation'\n",
        "filtered_dir = 'filtered-dogs'\n",
        "filtered_annotations_dir = os.path.join(filtered_dir, 'Annotation')\n",
        "filtered_images_dir = os.path.join(filtered_dir, 'Images')\n",
        "\n",
        "# Ensure the filtered directory is clean\n",
        "if os.path.exists(filtered_dir):\n",
        "    shutil.rmtree(filtered_dir)\n",
        "os.makedirs(filtered_annotations_dir)\n",
        "os.makedirs(filtered_images_dir)\n",
        "\n",
        "# List all species\n",
        "all_species = os.listdir(dataset_dir)\n",
        "print(f'Total species: {len(all_species)}')\n",
        "\n",
        "# Randomly select 5 species\n",
        "selected_species = random.sample(all_species, 5)\n",
        "print(f'Selected species: {selected_species}')\n",
        "\n",
        "# Copy selected species to the filtered directory\n",
        "for species in selected_species:\n",
        "    species_image_path = os.path.join(dataset_dir, species)\n",
        "    species_annotation_path = os.path.join(annotations_dir, species)\n",
        "    target_image_path = os.path.join(filtered_images_dir, species)\n",
        "    target_annotation_path = os.path.join(filtered_annotations_dir, species)\n",
        "\n",
        "    # Copy image files\n",
        "    shutil.copytree(species_image_path, target_image_path)\n",
        "\n",
        "    # Copy annotation files\n",
        "    shutil.copytree(species_annotation_path, target_annotation_path)\n",
        "\n",
        "print(\"Selected species images and annotations have been copied to the filtered directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96qfhzhClQpa",
        "outputId": "0f80bbbb-19c9-4e96-bb0d-d4e0e08090bd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total species: 120\n",
            "Selected species: ['n02086240-Shih-Tzu', 'n02101006-Gordon_setter', 'n02113712-miniature_poodle', 'n02104365-schipperke', 'n02097658-silky_terrier']\n",
            "Selected species images and annotations have been copied to the filtered directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generators\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=[0.2, 1.0],\n",
        "    channel_shift_range=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "988KQilOlWrj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = filtered_images_dir  # Use the filtered images directory"
      ],
      "metadata": {
        "id": "qjvfdy3NlbAy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    images_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5-BgH2fldpK",
        "outputId": "3516771c-f30b-43dd-9bff-d1bb38b0e984"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 690 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = datagen.flow_from_directory(\n",
        "    images_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0EgVqOXliNz",
        "outputId": "ffc9934c-33b6-4d97-e9a1-8a79c4ee5c0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 169 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    images_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbkPhUuKljX8",
        "outputId": "1dcd0c9c-9597-4440-f63d-39d038c0d966"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 859 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some information about the generators\n",
        "print(f'Total training samples: {train_generator.samples}')\n",
        "print(f'Total validation samples: {validation_generator.samples}')\n",
        "print(f'Total test samples: {test_generator.samples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nR3c_EDllfT",
        "outputId": "994c3ae1-28d7-4f48-e35b-7dd42a8df747"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training samples: 690\n",
            "Total validation samples: 169\n",
            "Total test samples: 859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "3un2caY4lolT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom CNN architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "v__ou4gNlrF8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with a smaller learning rate\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OuymaF0tltlC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "D62iMiYTlvjj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPk24UwXlxQc",
        "outputId": "de40a9bd-a013-4640-886a-9c61abf7c52e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 3.9001 - accuracy: 0.2204 - val_loss: 3.5908 - val_accuracy: 0.2266\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 85s 8s/step - loss: 3.3677 - accuracy: 0.2556 - val_loss: 3.0939 - val_accuracy: 0.2891\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 84s 8s/step - loss: 2.9209 - accuracy: 0.2764 - val_loss: 2.7186 - val_accuracy: 0.2500\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 93s 9s/step - loss: 2.5783 - accuracy: 0.2907 - val_loss: 2.4360 - val_accuracy: 0.2734\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 84s 8s/step - loss: 2.3283 - accuracy: 0.3083 - val_loss: 2.2120 - val_accuracy: 0.3125\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 2.1381 - accuracy: 0.3211 - val_loss: 2.0528 - val_accuracy: 0.3047\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 85s 8s/step - loss: 2.0169 - accuracy: 0.3003 - val_loss: 1.9935 - val_accuracy: 0.2812\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 1.9045 - accuracy: 0.3403 - val_loss: 1.8532 - val_accuracy: 0.2812\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 91s 9s/step - loss: 1.8435 - accuracy: 0.3211 - val_loss: 1.8007 - val_accuracy: 0.3203\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 1.7832 - accuracy: 0.3387 - val_loss: 1.7646 - val_accuracy: 0.3438\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 91s 9s/step - loss: 1.7306 - accuracy: 0.3797 - val_loss: 1.7229 - val_accuracy: 0.3828\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 87s 9s/step - loss: 1.7219 - accuracy: 0.3498 - val_loss: 1.7088 - val_accuracy: 0.3203\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 85s 8s/step - loss: 1.6820 - accuracy: 0.3626 - val_loss: 1.7383 - val_accuracy: 0.2969\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 98s 10s/step - loss: 1.6502 - accuracy: 0.3626 - val_loss: 1.6822 - val_accuracy: 0.3281\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 1.6717 - accuracy: 0.3355 - val_loss: 1.6459 - val_accuracy: 0.3594\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 91s 9s/step - loss: 1.6451 - accuracy: 0.3754 - val_loss: 1.6362 - val_accuracy: 0.3828\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 88s 9s/step - loss: 1.6204 - accuracy: 0.3786 - val_loss: 1.6797 - val_accuracy: 0.3438\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 1.5963 - accuracy: 0.3786 - val_loss: 1.6412 - val_accuracy: 0.3359\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 97s 10s/step - loss: 1.5693 - accuracy: 0.4073 - val_loss: 1.6270 - val_accuracy: 0.2891\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 85s 8s/step - loss: 1.5989 - accuracy: 0.3674 - val_loss: 1.5814 - val_accuracy: 0.3203\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 1.5898 - accuracy: 0.3797 - val_loss: 1.6216 - val_accuracy: 0.3594\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 87s 9s/step - loss: 1.5696 - accuracy: 0.3674 - val_loss: 1.6171 - val_accuracy: 0.2969\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 1.5553 - accuracy: 0.4281 - val_loss: 1.5729 - val_accuracy: 0.4062\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 94s 9s/step - loss: 1.5294 - accuracy: 0.3914 - val_loss: 1.5311 - val_accuracy: 0.4062\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 88s 9s/step - loss: 1.5640 - accuracy: 0.3530 - val_loss: 1.5512 - val_accuracy: 0.4375\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 1.5458 - accuracy: 0.3962 - val_loss: 1.5348 - val_accuracy: 0.4141\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 88s 9s/step - loss: 1.5564 - accuracy: 0.3850 - val_loss: 1.5706 - val_accuracy: 0.3750\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 101s 10s/step - loss: 1.5322 - accuracy: 0.4234 - val_loss: 1.5310 - val_accuracy: 0.4219\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 91s 9s/step - loss: 1.5052 - accuracy: 0.4073 - val_loss: 1.5214 - val_accuracy: 0.3672\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 91s 9s/step - loss: 1.5245 - accuracy: 0.3770 - val_loss: 1.5393 - val_accuracy: 0.3906\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 87s 9s/step - loss: 1.5399 - accuracy: 0.3946 - val_loss: 1.4918 - val_accuracy: 0.4453\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 88s 9s/step - loss: 1.5068 - accuracy: 0.4249 - val_loss: 1.5859 - val_accuracy: 0.4297\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 87s 9s/step - loss: 1.5034 - accuracy: 0.4169 - val_loss: 1.6292 - val_accuracy: 0.3438\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 96s 10s/step - loss: 1.5098 - accuracy: 0.4089 - val_loss: 1.4564 - val_accuracy: 0.4766\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 1.4884 - accuracy: 0.4422 - val_loss: 1.4953 - val_accuracy: 0.3984\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 87s 9s/step - loss: 1.4896 - accuracy: 0.4359 - val_loss: 1.5535 - val_accuracy: 0.4062\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 88s 9s/step - loss: 1.5055 - accuracy: 0.4058 - val_loss: 1.5097 - val_accuracy: 0.4062\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 90s 9s/step - loss: 1.4710 - accuracy: 0.4026 - val_loss: 1.4998 - val_accuracy: 0.4141\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 89s 9s/step - loss: 1.4686 - accuracy: 0.4425 - val_loss: 1.5164 - val_accuracy: 0.4453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f'Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvq3BTjml1gD",
        "outputId": "dec38507-aba4-4b09-ed27-50ff3efe3bba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 30s 2s/step - loss: 1.2887 - accuracy: 0.5460\n",
            "Test accuracy: 0.5459837317466736\n"
          ]
        }
      ]
    }
  ]
}